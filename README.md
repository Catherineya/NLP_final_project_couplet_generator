<img width="1191" alt="image" src="https://github.com/user-attachments/assets/25a85e17-69f9-4ade-9efa-5929ce36d8af" /># NLP_final_project_couplet_generator
 
# ğŸ§¾ Chinese Couplet Generator Based on Transformer

## ğŸ§  Overview

This project presents a **Transformer-based Chinese Couplet Generator**, designed to automatically generate the **lower line** of a traditional Chinese couplet when given the **upper line**. The model aims to preserve the syntactic symmetry and cultural aesthetics of classical Chinese duilian (å¯¹è”), widely used during festivals like the Lunar New Year.

## ğŸ—ï¸ Project Structure

- `chinese-couplets-generator-based-on-transformer.ipynb`: Main Jupyter notebook for training and generating couplets.
- `data/`: Folder for the training dataset (not included in this repo).
- `vocab.py`: Vocabulary building and token management.
- `model.py`: Transformer-based Encoder-Decoder architecture with attention.
- `utils.py`: Helper functions for training and generation (e.g., padding, batching).
- `Final_Project_Paper_for_Natural_Language_Processing.pdf`: Full project report.

## ğŸ§° Dependencies

```bash
pip install torch numpy tqdm matplotlib notebook
```

## ğŸ“š Dataset

We used a cleaned Chinese couplet dataset containing over **74,000 lines**, excluding vulgar content. Each data point consists of an upper line and a corresponding lower line.

Dataset split:
- **Training set**: 80%
- **Validation set**: 20%

Tokenization includes:
- `<PAD>`: Padding
- `<BOS>`: Beginning of Sentence
- `<EOS>`: End of Sentence
- `<UNK>`: Unknown token

## ğŸ§  Model Architecture

- Transformer Encoder-Decoder architecture with:
  - Token & Positional Embeddings
  - Multi-head Attention Layers
  - Cross-Attention Decoder
- Loss Function: Cross-Entropy Loss
- Optimizer: Adam

## ğŸš€ Training

Training was done at 10, 20, 30, and 40 epochs with adjustable batch size and learning rate.

### Example:
```python
for epoch in range(num_epochs):
    train(model, train_loader)
    val_loss = evaluate(model, val_loader)
```

## ğŸ“Š Evaluation

### Quantitative:
| Epoch | Perplexity | Cross-Entropy |
|-------|------------|----------------|
| 10    | 27.44      | 3.39           |
| 30    | 22.95      | 3.09           |
| 40    | 22.29      | 3.02           |

### Human Evaluation:
- Blind tests with native speakers comparing generated vs. real lines.
- Ratings on syntactic symmetry, semantic fluency, and overall impression (scale: 1â€“5).

## ğŸŒŸ Sample Results

| Upper Line | Generated Lower Line (Epoch 40) |
|------------|---------------------------------|
| å¯¹æœˆèµè·åŸé£éŸµ | ä¸´é£å¬é›¨å¬é›¨å£° |
| ä¸´é£æŠ«æ™“æœˆ | ä¸´æ°´å¬æ¸…éŸ³ |
| åƒå¤åå ‚å¥‰å›å­ | å®—è‡£é—åƒè‚ƒæ¸…é«˜ |

More in [`Final_Project_Paper_for_Natural_Language_Processing.pdf`](./Final_Project_Paper_for_Natural_Language_Processing.pdf).

## ğŸ”® Future Improvements

- Reduce character repetition in long lines
- Add rhyme and tone pattern alignment
- Develop automated stylistic scoring

## ğŸ‘¨â€ğŸ’» Contributors

- **Jianing Zhang** â€“ jz5212@nyu.edu  
- **Haotong Wu** â€“ hw2933@nyu.edu  
- **Xinwei Xie** â€“ xx2185@nyu.edu
